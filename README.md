
---

### **第一部分：基础概念与模型架构**

#### **第 1 章：核心概念与架构选择 (1-15)**
1.  什么是预测/判别式AI与生成式AI的区别？
2.  什么是大语言模型？LLM是如何训练的？
3.  语言模型中的“标记”是什么？
4.  Transformer架构是什么？它在LLM中是如何使用的？
5.  解释Encoder-only、Decoder-only、Encoder-Decoder架构的区别。 **[原4]**
6.  为什么目前LLM几乎全部采用Decoder-only架构？ **[原5]**
7.  为什么Transformer需要位置编码？ **[原9]**
8.  CNNs和RNNs不使用位置嵌入。为什么Transformer需要位置嵌入？
9.  为什么Transformer弃用RNN的循环结构？ **[原30]**
10. 解释Transformer模型如何解决CNN和RNN的局限性？
11. Transformer相对于LSTM有哪些优势？
12. 解释什么是自回归模型与掩码语言模型有何区别？
13. 解释什么是“深而窄”与“浅而宽”模型的优劣权衡？ **[原35]**
14. 解释不同LLM架构的类型，以及哪种架构最适合哪些任务？
15. 比较状态空间模型（如Mamba）与Transformer。各自的权衡及适用场景是什么？

#### **第 2 章：注意力机制变体 (16-40)**
16. Self-attention 的 $Q, K, V$ 矩阵分别代表什么？ **[原1]**
17. **[代码]** 实现一个标准的 Scaled Dot-Product Attention。 **[原2]**
18. 为什么 $QK^T$ 需要除以 $\sqrt{d}$？请从方差偏移的角度解释。 **[原3]**
19. 解释LLM中的注意力机制及其实现方式。
20. 什么是Multi-head Attention (MHA)？其并行化的核心逻辑是什么？ **[原14]**
21. 什么是Multi-query Attention (MQA)？它如何节省显存？ **[原15]**
22. 什么是Grouped-query Attention (GQA)？ **[原16]**
23. **[新增]** 什么是分组查询注意力中“组”的大小对性能的影响？
24. 解释注意力机制中的Causal Mask原理。 **[原17]**
25. 什么是Look-ahead Mask？ **[原39]**
26. 解释什么是Cross-attention。 **[原25]**
27. 解码器中的自注意力为何被称为交叉注意力？它与编码器中的自注意力有何不同？
28. 什么是Padding Token？它在Attention计算中如何被忽略？ **[原26]**
29. 什么是“注意力分数的数值稳定性”？ **[原33]**
30. 解释注意力矩阵的稀疏化。 **[原38]**
31. 解释什么是自注意力机制中的“感受野”。 **[原40]**
32. 解释什么是自注意力的“低秩性”。 **[原48]**
33. 什么是Sliding Window Attention？ **[原49]**
34. 解释什么是“注意力头”的冗余性。 **[原54]**
35. **[新增]** 解释Multi-head Latent Attention (MLA)的原理及其如何压缩KV Cache。 **[校: ★★★ | 社: ★★★★★]**
36. **[新增]** 什么是DeepSeek-V2中的吸收式注意力？ **[校: ★ | 社: ★★★]**
37. 解释局部注意力和全局注意力的区别。
38. 是什么使Transformer在计算和内存上开销巨大？我们如何解决？
39. 解释Flash Attention的分块计算原理。 **[原138]**
40. 解释Flash Attention在减少内存瓶颈方面的作用。

#### **第 3 章：位置编码与长文本建模 (41-55)**
41. 什么是位置编码？ **[原9]**
42. 解释绝对位置编码与相对位置编码的区别。 **[原10]**
43. 详细说明旋转位置编码 (RoPE) 的物理意义。 **[原11]**
44. 为什么RoPE具有长度外推性？ **[原12]**
45. 解释ALiBi位置编码的原理。 **[原13]**
46. 什么是Sinusoidal位置编码？ **[原37]**
47. 什么是可学习的位置编码？ **[原41]**
48. 现代LLM的位置编码是如何工作的？为什么旋转位置嵌入 (RoPE) 成了标准？
49. 如何增加LLM的上下文长度？
50. 解释什么是长距离依赖建模的极限。 **[原58]**
51. 什么是长文本外推中的插值法。 **[原227]**
52. 解释什么是“大海捞针”测试。 **[原228]**
53. 解释长序列推理中的FlashDecoding。 **[原246]**
54. 解释什么是长文本上下文压缩。 **[原238]**
55. LLM如何处理文本中的长期依赖关系？

#### **第 4 章：Norm、FFN与底层组件 (56-80)**
56. 什么是Pre-norm和Post-norm？为什么大模型偏向Pre-norm？ **[原6]**
57. **[代码]** 实现一个RMSNorm层。 **[原7]**
58. 解释LayerNorm的$\epsilon$项？ **[原45]**
59. 解释Residual Connection如何解决深层网络的梯度消失。 **[原19]**
60. 解释残差连接后的增益系数缩放。 **[原56]**
61. 解释SwiGLU激活函数的原理。 **[原8]**
62. 什么是Transformer中的FFN？它的参数量占比如何？ **[原18]**
63. 解释Transformer内部的维度扩张倍数（通常为4）的由来。 **[原36]**
64. 什么是词嵌入 (Embedding)？它在计算图中是怎样的一层？ **[原20]**
65. 解释Embedding维度的选择。 **[原21]**
66. 什么是Tying Embedding（权重共享）？ **[原31]**
67. 解释Embedding层的梯度更新频率。 **[原52]**
68. 解释什么是“深层特征与浅层特征”的融合。 **[原55]**
69. 解释词表映射层（Linear Head）的计算开销。 **[原42]**
70. 什么是模型的“瓶颈层”？ **[原43]**
71. 解释大模型中的“偏置项 (Bias)”为什么被很多架构取消？ **[原44]**
72. 解释Transformer处理变长输入的原理。 **[原46]**
73. 什么是Transformer中的并行层设计（如GPT-J）？ **[原53]**
74. 解释Transformer模型中的位置前馈网络子层的作用？
75. 什么是各向异性问题。 **[原23]**
76. 解释注意力层如何聚焦在输入的正确部分？
77. 什么是LayerNorm在推理阶段是否可以被优化融合？ **[原27]**
78. 解释计算图在反向传播时如何处理Dropout层。 **[原32]**
79. 解释Transformer模型如何解决梯度消失问题？
80. 解释Transformer模型中的残差连接的作用？

#### **第 5 章：混合专家模型 (MoE) (81-95)**
81. **[新增]** 解释Mixture-of-Experts (MoE)的基本原理与门控机制 (Gating)。 **[校: ★★★ | 社: ★★★★★]**
82. **[新增]** 什么是Expert Capacity和Token Dropping？ **[校: ★★ | 社: ★★★★]**
83. **[新增]** 解释负载均衡损失 (Load Balancing Loss) 在MoE训练中的作用。 **[校: ★★★ | 社: ★★★★]**
84. **[新增]** 什么是DeepSeek-V3中的“细粒度专家”？ **[校: ★★ | 社: ★★★★★]**
85. **[新增]** 解释无辅助损失的负载均衡策略。 **[校: ★ | 社: ★★★★]**
86. **[新增]** 什么是MoE中的Shared Experts（共享专家）？它解决了什么问题？ **[校: ★★ | 社: ★★★★]**
87. **[新增]** 解释专家并行 (Expert Parallelism) 与数据并行的区别。 **[校: ★★ | 社: ★★★★]**
88. **[新增]** 为什么MoE模型在推理阶段的显存占用大于激活参数量的稠密模型？ **[校: ★★★ | 社: ★★★★★]**
89. 解释权重矩阵的分块计算。 **[原50]**
90. 什么是分布式下的负载均衡。 **[原139]**
91. 什么是模型内部的数值溢出风险？ **[原59]**
92. 什么是内存屏障。 **[原57]**
93. 解释大模型中的“零碎算子”优化？ **[原51]**
94. 什么是混合专家模型？如何优化其推理效率？
95. 解释混合专家模型的基本原理及其在LLM预训练中的作用。

---

### **第二部分：数据工程、分词与预训练**

#### **第 6 章：分词器 (Tokenization) (96-115)**
96. 解释BPE (Byte Pair Encoding) 的工作流程。 **[原61]**
97. **[代码]** 模拟实现一个简单的BPE合并逻辑。 **[原62]**
98. 比较BPE、WordPiece和Unigram。 **[原63]**
99. 什么是Tokenizer的“压缩比”？ **[原64]**
100. 什么是Byte-level BPE？它解决了什么问题？ **[原65]**
101. 解释什么是OOV (Out-of-Vocabulary)。 **[原66]**
102. 解释Tokenizer的词表扩容对预训练模型的影响。 **[原76]**
103. 什么是Special Tokens（如 BOS, EOS, PAD）？ **[原77]**
104. 如何评估一个Tokenizer的质量？ **[原78]**
105. 什么是词表的频率阈值？ **[原96]**


#### **第 7 章：数据清洗、去重与合成 (116-140)**
116. 什么是预训练数据清洗中的“启发式规则”？ **[原67]**
117. 如何识别并过滤网页抓取数据中的机器翻译感语料？ **[原68]**
118. 解释MinHash和LSH在大规模数据去重中的应用。 **[原69]**
119. 什么是预训练中的“数据污染”检测？ **[原75]**
120. 什么是数据处理中的PII脱敏？ **[原79]**
121. 什么是数据清洗中的“困惑度 (Perplexity) 过滤”？ **[原84]**
122. 什么是数据处理中的正则表达式应用。 **[原89]**
123. 什么是网页数据的Markdown化处理？ **[原90]**


#### **第 8 章：数据配比、课程学习与缩放法则 (141-155)**
141. 什么是“数据配比 (Data Mixture)”？ **[原70]**
142. 为什么代码数据对提升逻辑推理能力至关重要？ **[原71]**
143. 如何处理多语言语料库中的语言不平衡？ **[原72]**
144. 什么是“数据退火 (Data Annealing)”？ **[原73]**

---

### **第三部分：分布式训练、优化与精度**

#### **第 9 章：分布式并行策略 (156-180)**
156. 详细解释ZeRO-1, ZeRO-2, ZeRO-3。 **[原114]**
157. 什么是张量并行 (TP)？请描述MLP层的切分。 **[原115]**
158. 什么是流水线并行 (PP)？解释1F1B调度。 **[原116]**
159. 解释流水线并行中的“气泡 (Bubble)”。 **[原117]**
160. 什么是数据并行 (DP) 与分布式数据并行 (DDP)？ **[原118]**
161. 解释什么是多维并行 (3D Parallelism)。 **[原119]**
162. **[新增]** 解释什么是序列并行 (Sequence Parallelism)？ **[校: ★★★ | 社: ★★★★★]**


#### **第 10 章：训练配置、优化器与稳定性 (181-200)**
181. 解释什么是预训练阶段的“学习率预热”。 **[原148]**
182. 什么是Adam优化器的显存代价？ **[原143]**
183. 解释什么是混合精度中的“主权重”。 **[原130]**
184. 什么是权重衰减 (Weight Decay)？ **[原131]**
185. 什么是超参数调优在大模型中的限制。 **[原147]**
186. 混合精度训练 (FP16/BF16) 的原理。 **[原120]**



---

### **第四部分：微调、对齐与推理模型**


#### **第 13 章：人类偏好对齐 (RLHF/DPO/KTO) (236-260)**
236. 解释RLHF的三阶段流程。 **[原171]**
237. 什么是奖励模型 (Reward Model)？ **[原172]**
238. 解释奖励模型训练中的Rank Loss。 **[原173]**
239. 什么是PPO算法？它的四个核心模型是什么？ **[原174]**
240. 为什么RLHF阶段需要KL散度约束？ **[原175]**
241. 什么是DPO (Direct Preference Optimization)？ **[原176]**

---

### **第五部分：推理、部署与应用系统**

#### **第 15 章：推理系统优化与解码 (281-310)**

300. 解释贪婪搜索解码策略及其主要缺点。
301. 波束搜索如何改进贪婪搜索，波束宽度参数的作用是什么？
302. 比较确定性和随机解码方法。
303. 解释解码策略对LLM生成输出质量和延迟的影响。
304. 什么是投机采样 (Speculative Decoding)？ **[原223]**


#### **第 19 章：评测、多模态、安全与部署实践 (376-400)**
376. **[新增]** 解释多模态模型中的连接器 (Projector) 作用（如LLaVA）。 **[校: ★★★ | 社: ★★★★]**
377. **[新增]** 什么是LLM-as-a-Judge？如何缓解位置偏见 (Position Bias)？ **[校: ★★★★ | 社: ★★★★★]**
378. 如何评估对齐后的模型性能？ **[原199]**
379. 如何衡量LLM的性能？（评估指标与基准测试）
380. 如何评估针对你用例的最佳LLM模型？
381. 评估LLM有哪些不同的指标？
382. 模型可解释性在LLM中为何重要？如何实现？
394. 解释模型部署中的“零拷贝”数据传输。 **[原250]**
395. 什么是大模型部署中的常见性能瓶颈与优化手段？
396. 如何构建生产级的LLM应用系统？
397. 什么是案例研究：具有动态上下文的LLM聊天助手？
398. 什么是案例研究：高级提示技术？
399. 解释什么是智能体 (Agent) 在LLM应用中的发展趋势。
400. 总结未来LLM技术发展的关键挑战与机遇。

为保护版权完整版将在后续发布,

### 作者 874953727@qq.com
 注意,部分内容使用AI参与了整理 @deepseek
